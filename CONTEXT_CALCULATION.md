# Context calculation

How token usage and cost are tracked per conversation.

---

## Overview

OpenCode tracks token usage and cost per session. There are two layers:

1. **Server-side** — normalizes raw provider usage into a standard format and computes costs, stored on each assistant message automatically.
2. **Client-side** — reads token data from the last assistant message and computes usage percentages and visual breakdowns.

---

## Data model

### Token breakdown

Every assistant message returned by the API contains `cost` and `tokens` fields.

```json
{
  "cost": 0.0123,
  "tokens": {
    "input": 15000,
    "output": 2000,
    "reasoning": 500,
    "cache": {
      "read": 3000,
      "write": 1500
    }
  }
}
```

| Field         | Description                                                           |
| ------------- | --------------------------------------------------------------------- |
| `input`       | Input tokens sent to the model, excluding cache tokens                |
| `output`      | Output tokens generated by the model                                  |
| `reasoning`   | Reasoning/thinking tokens (Claude extended thinking, OpenAI o-series) |
| `cache.read`  | Tokens read from the provider's prompt cache                          |
| `cache.write` | Tokens written to the provider's prompt cache                         |

Each `step-finish` part within a message also carries per-step `tokens` and `cost`.

---

### Model limits

Each model has a `limit` object from the provider endpoint.

```json
{
  "limit": {
    "context": 200000,
    "input": 180000,
    "output": 16384
  }
}
```

| Field     | Description                              |
| --------- | ---------------------------------------- |
| `context` | Total context window size in tokens      |
| `input`   | Optional explicit input token limit      |
| `output`  | Maximum output tokens the model supports |

The `input` field may not be present. When absent, the usable input limit is `context - output`.

---

### Model cost

Each model has cost rates per million tokens.

```json
{
  "cost": {
    "input": 3.0,
    "output": 15.0,
    "cache": {
      "read": 0.3,
      "write": 3.75
    },
    "experimentalOver200K": {
      "input": 6.0,
      "output": 30.0,
      "cache": { "read": 0.6, "write": 7.5 }
    }
  }
}
```

The `experimentalOver200K` rates kick in when `input + cache.read > 200,000` tokens.

---

## API endpoints

### Fetch messages

`GET /session/:sessionID/message`

Returns `MessageV2.WithParts[]`. Each assistant message has `cost` and `tokens` fields already computed by the server.

---

### Fetch providers

`GET /provider/`

Returns `{ all: Provider[], default: Record<string, string>, connected: string[] }`. Each provider contains a `models` record where each model has `limit` and `cost`.

---

### Subscribe to events

`GET /event`

SSE stream that broadcasts `message.updated` and `message.part.updated` events in real-time as the LLM generates responses, including updated token/cost data.

---

## Compute total tokens

Sum all five categories from the last assistant message with tokens > 0.

```ts
const total = tokens.input + tokens.output + tokens.reasoning + tokens.cache.read + tokens.cache.write
```

This represents how much of the context window the conversation is using.

---

## Compute usage percentage

```ts
const usage = Math.round((total / model.limit.context) * 100)
```

If the model's context limit is unknown (`0` or `undefined`), return `null`.

---

## Find the right message

Walk backward through the message array to find the last assistant message that has tokens > 0. Some assistant messages may have 0 tokens (e.g. compaction summaries during initialization).

```ts
function lastAssistantWithTokens(messages) {
  for (let i = messages.length - 1; i >= 0; i--) {
    const msg = messages[i]
    if (msg.role !== "assistant") continue
    if (tokenTotal(msg) <= 0) continue
    return msg
  }
}
```

---

## Compute session cost

Sum the `cost` field across all assistant messages in the session.

```ts
const totalCost = messages.filter((msg) => msg.role === "assistant").reduce((sum, msg) => sum + msg.cost, 0)
```

---

## Resolve the model

Each assistant message has `providerID` and `modelID`. Look these up from the provider list.

```ts
const provider = providers.find((p) => p.id === message.providerID)
const model = provider?.models[message.modelID]
const limit = model?.limit.context
```

Use fallback labels if metadata isn't found:

```ts
const providerLabel = provider?.name ?? message.providerID
const modelLabel = model?.name ?? message.modelID
```

---

## Build a breakdown bar (optional)

The existing client shows an approximate breakdown of what's consuming input tokens. This is purely a UI estimation, not exact data from the server.

### Estimate tokens from content

Walk through all messages and their parts. Sum character lengths by category, then convert to estimated tokens.

```ts
const estimatedTokens = Math.ceil(characters / 4)
```

The 4 chars/token ratio is a rough heuristic used throughout the codebase.

---

### Categories

| Category  | Color  | Source                                             |
| --------- | ------ | -------------------------------------------------- |
| System    | Blue   | System prompt characters                           |
| User      | Green  | User text parts, file parts, agent parts           |
| Assistant | Purple | Assistant text + reasoning parts                   |
| Tool      | Orange | Tool input/output characters                       |
| Other     | Gray   | Unaccounted tokens (internal overhead, formatting) |

---

### Scale to fit actuals

If the estimated total exceeds the actual `input` token count from the last assistant message, scale all categories proportionally.

```ts
if (estimated > actualInput) {
  const scale = actualInput / estimated
  each = Math.floor(category * scale)
}
const other = actualInput - sumOfScaledCategories
```

The "Other" category captures the difference between estimated and actual input tokens.

---

## Server-side reference

You don't need to implement any of this — the server handles it. But it helps to understand what's happening behind the scenes.

### Token normalization

Different providers report tokens differently.

**Anthropic/Bedrock** report `inputTokens` _excluding_ cached tokens. Cache values come from separate metadata fields.

**Other providers** (OpenAI, Google, etc.) report `inputTokens` _including_ cached tokens. The server subtracts cache from the raw count.

```
if (anthropic or bedrock):
    adjustedInput = rawInputTokens
else:
    adjustedInput = rawInputTokens - cacheRead - cacheWrite
```

---

### Cost formula

Cost is computed per million tokens using decimal math.

```
cost = (input × rate.input / 1,000,000)
     + (output × rate.output / 1,000,000)
     + (cache.read × rate.cache.read / 1,000,000)
     + (cache.write × rate.cache.write / 1,000,000)
     + (reasoning × rate.output / 1,000,000)
```

Reasoning tokens are charged at the **output** token rate. If `input + cache.read > 200,000` and the model has `experimentalOver200K` pricing, the higher rates are used.

---

### Overflow detection

The server checks if the context window is full after each step.

```
count = input + cache.read + output
outputReserve = min(model.limit.output, 32000) || 32000
usable = model.limit.input || (model.limit.context - outputReserve)
isOverflow = count > usable
```

The default output reserve is `32000`, overridable via `OPENCODE_EXPERIMENTAL_OUTPUT_TOKEN_MAX`. When overflow is detected, the server triggers compaction (conversation summarization).

---

### Pruning

The server prunes old tool outputs to reduce context size.

| Constant        | Value         | Purpose                                      |
| --------------- | ------------- | -------------------------------------------- |
| `PRUNE_PROTECT` | 40,000 tokens | Keep the last 40K of tool output untouched   |
| `PRUNE_MINIMUM` | 20,000 tokens | Only prune if there's at least 20K to remove |

It walks backward through messages, estimates tool output sizes using the 4 chars/token heuristic, and clears old tool outputs. Protected tools (e.g. `skill`) are never pruned.

---

## Implementation checklist

1. **Fetch providers** from `GET /provider/` to get model limits and costs
2. **Fetch messages** from `GET /session/:id/message` to get token/cost data
3. **Subscribe to SSE** at `GET /event` for real-time updates
4. **Find last assistant message** with tokens > 0 (walk backward)
5. **Compute total tokens** — sum all 5 categories
6. **Compute usage %** — `Math.round((total / model.limit.context) * 100)`
7. **Compute total cost** — sum `.cost` across all assistant messages
8. **Display** — show usage percentage, total tokens, and cost
9. **(Optional)** Build breakdown bar using 4 chars/token heuristic on message content

---

## Source files

| File                                                             | Purpose                                      |
| ---------------------------------------------------------------- | -------------------------------------------- |
| `packages/app/src/components/session/session-context-metrics.ts` | Client-side context calculation              |
| `packages/app/src/components/session-context-usage.tsx`          | Context usage UI component                   |
| `packages/app/src/components/session/session-context-tab.tsx`    | Full context detail panel with breakdown bar |
| `packages/opencode/src/session/index.ts`                         | Server-side token normalization and cost     |
| `packages/opencode/src/session/compaction.ts`                    | Overflow detection and compaction            |
| `packages/opencode/src/session/processor.ts`                     | Usage accumulation during streaming          |
| `packages/opencode/src/provider/provider.ts`                     | Model schema with limits and costs           |
| `packages/opencode/src/util/token.ts`                            | Token estimation heuristic                   |
| `packages/opencode/src/provider/error.ts`                        | Context overflow error patterns              |
